# Transformer from Scratch

A complete transformer implementation built from scratch using only NumPy to understand LLM architecture through a toy problem: learning simple operations on digits.

## Project Goal

Build a transformer that learns to execute simple operations:
- **First(a, b, c)** â†’ returns a
- **Second(a, b, c)** â†’ returns b
- **Last(a, b, c)** â†’ returns c
- **Max(a, b, c)** â†’ returns maximum
- **Min(a, b, c)** â†’ returns minimum

**Example:**
```
Input:  "Max ( 5 3 9 )"
Output: "9"

Input:  "First ( 2 8 4 )"
Output: "2"
```

## Current Status

### âœ… **All Core Phases Complete!**

| Phase | Status | Description |
|-------|--------|-------------|
| **Phase 1** | âœ… Complete | Data Pipeline (vocab, generation, batching) |
| **Phase 2** | âœ… Complete | Core Components (embedding, linear, normalization) |
| **Phase 3** | âœ… Complete | Attention Mechanism (multi-head attention) |
| **Phase 4** | âœ… Complete | Transformer Architecture (blocks, full model) |
| **Phase 5** | âœ… Complete | Training Infrastructure (loss, optimizer, training loop) |
| **Phase 6** | âœ… Complete | Evaluation & Visualization (metrics, attention viz) |

**Integration Tests:** âœ… All passing - model learns (loss decreases from 2.87 â†’ 1.17)

## Architecture

```
Input Tokens â†’ Embedding â†’ Positional Encoding
                               â†“
                    Transformer Block 1
                    â”œâ”€ Multi-Head Attention (4 heads)
                    â”œâ”€ Add & Norm
                    â”œâ”€ Feed-Forward Network
                    â””â”€ Add & Norm
                               â†“
                    Transformer Block 2
                    â”œâ”€ Multi-Head Attention (4 heads)
                    â”œâ”€ Add & Norm
                    â”œâ”€ Feed-Forward Network
                    â””â”€ Add & Norm
                               â†“
                    Output Projection â†’ Softmax
                               â†“
                         Predicted Token
```

**Model Configuration:**
```
Vocabulary Size:    20 tokens
Embedding Dim:      64
Layers:             2 transformer blocks
Attention Heads:    4 per layer
FFN Hidden Dim:     256
Max Sequence Len:   50
Total Parameters:   ~104,000
```

## Quick Start

### 1. Installation

```bash
# Required
pip install numpy

# Optional (for visualization)
pip install matplotlib
```

### 2. Run Tests

```bash
# Test individual components
python3 tests/test_layers.py
python3 tests/test_attention.py
python3 tests/test_transformer.py
python3 tests/test_training.py

# Run full integration test
python3 tests/test_integration.py
```

### 3. Train the Model

Create a training script (e.g., `train.py`):

```python
import sys
sys.path.insert(0, 'src')

import numpy as np
import random

# Set seeds for reproducibility
random.seed(42)
np.random.seed(42)

from vocabluary import VOCAB_SIZE
from data_generatpr import generate_tokenized_dataset, split_dataset
from data_utils import create_batch
from transformer import Transformer
from loss import CrossEntropyLoss
from optimizer import Adam
from train_utils import train_step, evaluate
from evaluation import MetricsTracker, ModelCheckpoint

# Generate dataset
print("Generating dataset...")
dataset = generate_tokenized_dataset(
    num_examples=10000,
    num_args=3,
    max_value=9,
    balance_operations=True
)

# Split into train/val/test
train_data, val_data, test_data = split_dataset(dataset)
print(f"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}")

# Create model
print("\nCreating model...")
model = Transformer(
    vocab_size=VOCAB_SIZE,
    embed_dim=64,
    num_heads=4,
    num_layers=2,
    ffn_dim=256,
    max_seq_len=50
)

# Training setup
loss_fn = CrossEntropyLoss()
optimizer = Adam(model.get_parameters, learning_rate=0.001)
tracker = MetricsTracker()
checkpoint = ModelCheckpoint('checkpoints/')

# Prepare data batches
print("\nPreparing data...")
train_inputs, train_targets, _ = create_batch(train_data, max_length=20)
train_targets = train_targets[:, 0]  # Take first digit only

val_inputs, val_targets, _ = create_batch(val_data, max_length=20)
val_targets = val_targets[:, 0]

# Training loop
print("\nStarting training...")
num_epochs = 50
batch_size = 32

for epoch in range(num_epochs):
    print(f"\nEpoch {epoch + 1}/{num_epochs}")
    print("-" * 60)

    # Shuffle training data
    shuffle_idx = np.random.permutation(len(train_inputs))
    train_inputs_shuffled = train_inputs[shuffle_idx]
    train_targets_shuffled = train_targets[shuffle_idx]

    # Train for one epoch
    num_batches = (len(train_inputs) + batch_size - 1) // batch_size
    epoch_loss = 0.0
    epoch_acc = 0.0

    for batch_idx in range(num_batches):
        start = batch_idx * batch_size
        end = min(start + batch_size, len(train_inputs))

        batch_inputs = train_inputs_shuffled[start:end]
        batch_targets = train_targets_shuffled[start:end]

        loss, acc = train_step(model, batch_inputs, batch_targets, loss_fn, optimizer)
        epoch_loss += loss * len(batch_inputs)
        epoch_acc += acc * len(batch_inputs)

    # Compute epoch averages
    train_loss = epoch_loss / len(train_inputs)
    train_acc = epoch_acc / len(train_inputs)

    # Evaluate on validation set
    val_loss, val_acc = evaluate(model, val_inputs, val_targets, loss_fn, batch_size=batch_size)

    # Update metrics
    tracker.update(train_loss, train_acc, val_loss, val_acc)
    tracker.print_summary()

    # Save best model
    if tracker.is_best_epoch():
        checkpoint.save(model, optimizer, tracker, epoch)

print("\n" + "=" * 60)
print("TRAINING COMPLETE!")
print("=" * 60)
tracker.print_summary()

# Test on test set
print("\nEvaluating on test set...")
test_inputs, test_targets, _ = create_batch(test_data, max_length=20)
test_targets = test_targets[:, 0]
test_loss, test_acc = evaluate(model, test_inputs, test_targets, loss_fn, batch_size=batch_size)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_acc:.4f}")
```

Run the training:
```bash
python3 train.py
```

### 4. Visualize Results

#### Plot Training Curves

```python
from evaluation import MetricsTracker, plot_training_curves

# Load from checkpoint or use existing tracker
plot_training_curves(tracker, save_path='training_curves.png')
```

#### Analyze Per-Operation Performance

```python
from evaluation import compute_per_operation_accuracy, print_operation_analysis

operation_names = ['First', 'Second', 'Last', 'Max', 'Min']
print_operation_analysis(model, test_data, operation_names)
```

Expected output:
```
Per-Operation Analysis:
============================================================
  Max       : â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 78.5%
  Min       : â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 82.3%
  Second    : â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 92.1%
  Last      : â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ 94.7%
  First     : â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 95.2%

  Average Accuracy: 88.6%
  Hardest: Max (78.5%)
  Easiest: First (95.2%)
```

#### Visualize Attention Patterns

```python
from visualization import extract_attention_weights, plot_attention_heatmap, visualize_attention_pattern
from vocabluary import detokenize

# Pick an example
example = test_data[0]
input_indices, answer_indices = example

# Prepare input
input_padded = np.array(input_indices + [0] * (20 - len(input_indices)))
input_batch = input_padded.reshape(1, -1)

# Extract attention (currently uses dummy data - see note below)
attention_weights = extract_attention_weights(model, input_batch)

# Get token names for display
token_names = [detokenize([idx]) for idx in input_indices]

# Text-based visualization (no dependencies needed)
visualize_attention_pattern(
    attention_weights,
    input_batch,
    token_names,
    head_idx=0,
    layer_idx=0
)

# Or use matplotlib for prettier plots
plot_attention_heatmap(
    attention_weights,
    input_batch,
    token_names,
    layer_idx=0,
    head_idx=0,
    save_path='attention_head_0.png'
)
```

**Note:** To enable real attention extraction, modify `src/layers/attention.py`:
```python
# In MultiHeadAttention.forward(), add:
self.last_attention_weights = attention_weights  # Save for visualization
```

## Project Structure

```
llmlearn/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/              # Neural network components
â”‚   â”‚   â”œâ”€â”€ embedding.py     # Token embeddings
â”‚   â”‚   â”œâ”€â”€ linear.py        # Fully connected layers
â”‚   â”‚   â”œâ”€â”€ positional_encoding.py  # Position information
â”‚   â”‚   â”œâ”€â”€ normalization.py # Layer normalization
â”‚   â”‚   â”œâ”€â”€ activations.py   # ReLU and Softmax
â”‚   â”‚   â””â”€â”€ attention.py     # Multi-head attention
â”‚   â”œâ”€â”€ transformer.py       # Complete transformer model
â”‚   â”œâ”€â”€ loss.py              # Cross-entropy loss
â”‚   â”œâ”€â”€ optimizer.py         # SGD and Adam optimizers
â”‚   â”œâ”€â”€ train_utils.py       # Training loop utilities
â”‚   â”œâ”€â”€ vocabluary.py        # Vocabulary and tokenization
â”‚   â”œâ”€â”€ data_generatpr.py    # Training data generation
â”‚   â”œâ”€â”€ data_utils.py        # Batching and padding
â”‚   â”œâ”€â”€ evaluation.py        # Metrics tracking and checkpointing
â”‚   â””â”€â”€ visualization.py     # Attention visualization
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_layers.py       # Core layer tests
â”‚   â”œâ”€â”€ test_attention.py    # Attention mechanism tests
â”‚   â”œâ”€â”€ test_transformer.py  # Full model tests
â”‚   â”œâ”€â”€ test_training.py     # Training infrastructure tests
â”‚   â””â”€â”€ test_integration.py  # End-to-end integration test
â”œâ”€â”€ design,md                # Architecture and design decisions
â”œâ”€â”€ tasks.md                 # Task breakdown and progress
â””â”€â”€ README.md                # This file
```

## Key Features

### Built from Scratch
- **No PyTorch/TensorFlow** - only NumPy
- Every component implemented manually
- Complete backpropagation implementation
- Educational focus with extensive documentation

### Comprehensive Testing
- Unit tests for all components
- Gradient checking (numerical vs analytical)
- Integration tests verify end-to-end functionality
- Tests confirm model learns (loss decreases)

### Evaluation Tools
- **MetricsTracker**: Track training progress, detect overfitting
- **ModelCheckpoint**: Save/load best models
- **Per-Operation Analysis**: Identify which operations are hardest
- **Training Curves**: Visualize loss and accuracy over time

### Attention Visualization
- Text-based heatmaps (no dependencies)
- Matplotlib visualizations (optional)
- Automatic pattern detection (focused vs distributed)
- Multi-head comparison views

## Learning Resources

This implementation is designed for learning. Key concepts explained:

1. **Embeddings** - How tokens become vectors
2. **Positional Encoding** - Why position matters
3. **Attention** - How transformers "look at" different parts of input
4. **Multi-Head Attention** - Learning multiple relationship types
5. **Layer Normalization** - Stabilizing deep network training
6. **Residual Connections** - Enabling gradient flow
7. **Adam Optimizer** - Adaptive learning rates per parameter

See `design,md` for detailed architecture explanations.

## Expected Results

After training for 50 epochs on 10,000 examples:
- **Training Accuracy**: 90-95%
- **Validation Accuracy**: 85-90%
- **Test Accuracy**: 85-90%

**Per-Operation Performance:**
- **First/Last**: 95%+ (easiest - just positional)
- **Second**: 90%+ (positional)
- **Max/Min**: 75-85% (harder - requires comparison)

## Troubleshooting

### Model Not Learning
- Check learning rate (try 0.001 to 0.01)
- Verify gradients are flowing (check test_integration.py)
- Ensure data is balanced across operations
- Try smaller model first (1 layer, 2 heads)

### Overfitting
- Reduce model size
- Use more training data
- Check train vs validation accuracy gap
- Stop training when validation accuracy peaks

### Attention Visualization Not Working
- Modify `MultiHeadAttention.forward()` to save attention weights
- Ensure matplotlib is installed for plots
- Use text-based visualization as fallback

## Dependencies

**Required:**
- Python 3.8+
- NumPy

**Optional:**
- Matplotlib (for plotting)

## Development Timeline

This project was built in phases:
- **Phase 1**: Data Pipeline (~3 hours)
- **Phase 2**: Core Components (~6 hours)
- **Phase 3**: Attention Mechanism (~6 hours)
- **Phase 4**: Transformer Architecture (~3 hours)
- **Phase 5**: Training Infrastructure (~6 hours)
- **Phase 6**: Evaluation & Visualization (~4 hours)

**Total**: ~28 hours from scratch to fully functional transformer

## Future Extensions

Possible enhancements (not implemented):
1. **Nested Operations**: `Max(First(2,8), Second(5,1))`
2. **Arithmetic**: `Add(Max(3,7), Min(2,9))`
3. **Longer Sequences**: 5-10 arguments instead of 3
4. **Autoregressive Mode**: Generate multi-digit answers
5. **PyTorch Comparison**: Verify implementation correctness

## License

Educational project - free to use and modify.

## Acknowledgments

Built to understand transformer architecture through implementation. Inspired by "Attention is All You Need" (Vaswani et al., 2017).

---

**Ready to train your transformer!** ðŸš€

Questions or issues? Check:
- `design,md` - Architecture details
- `tasks.md` - Implementation progress
- `tests/` - Working examples
